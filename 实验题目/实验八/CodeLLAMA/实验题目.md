# 实验题目
熟悉Code Llama代码生成的基本原理，能够在DLP上实现代码生成、代码补全、指令微调一系列功能，学会使用Triton语言进行Flash Attention算子的封装并对Code Llama模型进行算子优化，具体包括：

1) 掌握基于Code Llama实现代码生成的基本原理和操作步骤；

2) 了解自定义Code Llama推理系统构建的关键步骤，完成Transformer模型构建模块、Llama代码生成模块，能够实现代码生成、代码补全以及指令微调一系列基本功能；

3) 能够实现基于Hugging Face Transformers库的Code Llama推理系统构建，并与自定义Code Llama推理系统构建流程进行比较；

4) 了解Triton语言进行Flash Attention算法实现的基本原理、算法流程；

5) 能够使用Flash Attention算子对Code Llama模型进行优化。

实验工作量：约300行代码，24小时。

# 实验环境

硬件平台：DLP云平台环境。

软件环境：编程框架Pytorch1.13.1、CNNL高性能AI运算库，CNRT 运行时库，以及 python 环境及相关的扩展库。

# 实验内容和步骤

详情请查看实验指导书

# 评分标准

•  60分标准：能够正确实现自定义Code Llama推理系统构建，正确实现Transformer模型构建模块、Llama代码生成模块。

•  70分标准：在60分标准的基础上，能够正确实现自定义Code Llama的代码生成、代码补全、指令微调功能。

•  80分标准：在70分标准基础上，能够基于Transformers库的Code Llama推理系统构建。 

•  90分标准：在80分的基础上，能够正确实现基于寒武纪Triton的Flash Attention算子封装，并能够正确实现单算子测试。

•  100分标准：在90分的基础上，能够正确完成Code Llama模型上的推理，使用Flash Attention算子正确实现基于Flash Attention算子的
             Code Llama的推理系统。

# 文件提交格式

需要提交的文件为Codellama-infer.py、llama_mlu/model.py、llama_mlu/generation.py、 example_completion_mlu.py、example_infilling_mlu.py、example_instructions_mlu.py、flash_attention_triton_opt.py以及/opt/tools/native/transformers_mlu/src/transformers/models/llama/modeling_llama.py，将上述文件直接打包为 zip 文件提交。